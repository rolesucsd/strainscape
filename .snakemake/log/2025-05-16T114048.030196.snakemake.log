Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                       count
----------------------  -------
all                           1
analyze_mutation_types        1
calculate_trends              1
map_genes                     1
merge_snv_info                1
prepare_trajectories          1
process_scaffolds             1
total                         7

Select jobs to execute...

[Fri May 16 11:40:48 2025]
rule process_scaffolds:
    input: /Users/reneeoles/Desktop/input/instrain/H4020/combined_scaffold_info.tsv, /Users/reneeoles/Desktop/input/assembly/H4020/assembly.stb, /Users/reneeoles/Desktop/input/hmp2_metadata_2018-08-20.csv, /Users/reneeoles/Desktop/input/bins/H4020/bin.txt
    output: /Users/reneeoles/Desktop/input/instrain/H4020/processed_scaffolds.txt
    log: /Users/reneeoles/Desktop/input/logs/process_scaffolds_H4020.log
    jobid: 4
    reason: Missing output files: /Users/reneeoles/Desktop/input/instrain/H4020/processed_scaffolds.txt
    wildcards: patient=H4020
    resources: tmpdir=/var/folders/mc/c6ndbtvn6kn2jyz7c6ymt42h0000gn/T, mem_mb=16000, mem_mib=15259, threads=4

RuleException in rule process_scaffolds in file /Users/reneeoles/Desktop/strainscape/snakemake/rules/snp_tracking.smk, line 22:
AttributeError: 'OutputFiles' object has no attribute 'processed_file', when formatting the following:

        python strainscape/process_scaffolds.py             --scaffold_file {input.scaffold_file}             --stb_file {input.stb_file}             --metadata_file {input.metadata_file}             --bin_file {input.bin_file}             --output_file {output.processed_file}             --min_length {params.min_length}             --min_coverage {params.min_coverage}             --min_breadth {params.min_breadth}             --log_file {log} 2>&1
        
