# Snakemake workflow for the iHMP pipeline

import os, re
import pandas as pd
from collections import defaultdict
from functools import lru_cache

# Load user config
configfile: "config/config.yaml"

# Base directories from config
BASE_DIR        = config["paths"]["data_dir"]
FASTQ_LIST_DIR  = config["paths"]["fastq_dir"]
LOG_DIR         = config["paths"]["log_dir"]
INSTR_DIRECT       = os.path.join(BASE_DIR, "instrain")
SAMPLE_GROUP_TSV = config["metadata"]["samples"]

# load mapping (columns: sample, group)
sg = pd.read_csv(SAMPLE_GROUP_TSV, sep=None, engine="python", dtype=str)
sg["sample"] = sg["sample"].str.strip()
sg["group"]  = sg["group"].str.strip()

GROUP_TO_SAMPLES = defaultdict(list)
for _, r in sg.iterrows():
    GROUP_TO_SAMPLES[r["group"]].append(r["sample"])
PATIENT_IDS = sorted(GROUP_TO_SAMPLES.keys())

@lru_cache(maxsize=None)
def read_samples(patient: str):
    """
    Build {sample: {"R1": path, "R2": path}} for samples that belong to `patient`.
    Assumes files are named {sample}_R1.fastq.gz and {sample}_R2.fastq.gz in FASTQ_LIST_DIR.
    """
    allowed = sorted(set(GROUP_TO_SAMPLES.get(patient, [])))
    out = {}
    missing = []
    for s in allowed:
        r1 = os.path.join(FASTQ_LIST_DIR, f"{s}_R1.fastq.gz")
        r2 = os.path.join(FASTQ_LIST_DIR, f"{s}_R2.fastq.gz")
        if os.path.exists(r1) and os.path.exists(r2):
            out[s] = {"R1": r1, "R2": r2}
        else:
            missing.append((s, r1, r2))
    if missing:
        msg = "\n".join([f"  - {s}\n      R1: {r1}\n      R2: {r2}" for s, r1, r2 in missing[:6]])
        raise FileNotFoundError(
            f"Missing FASTQs for {len(missing)} sample(s) in {FASTQ_LIST_DIR}.\n{msg}"
            + ("\n  ..." if len(missing) > 6 else "")
        )
    return out

# Per-sample paths
MAP_BAM    = lambda p, s: os.path.join(BASE_DIR, "mapping", p, f"{s}.all.bam")
SORT_BAM   = lambda p, s: os.path.join(BASE_DIR, "mapping", p, f"{s}.filtered.sorted.bam")
FLAGSTAT   = lambda p, s: os.path.join(BASE_DIR, "mapping", p, f"{s}.filtered.flagstat.txt")
SNV_FILE   = lambda p, s: os.path.join(BASE_DIR, "instrain", p, "each", s, "output", f"{s}_SNVs.tsv")
SCF_FILE   = lambda p, s: os.path.join(BASE_DIR, "instrain", p, "each", s, "output", f"{s}_scaffold_info.tsv")

# Flatten list of all flagstat files
def all_flagstats():
    return [FLAGSTAT(p, s) for p in PATIENT_IDS for s in read_samples(p)]

# Include shared snippets and rules
include: "../strainscape/utils.py"
include: "../strainscape/wildcards.smk"
include: "rules/assembly.smk"
include: "rules/mapping.smk"
include: "rules/mags.smk"
include: "rules/instrain.smk"
include: "rules/snp_tracking.smk"

# Main pipeline target: all outputs
rule all:
    input:
        # per-patient outputs
        expand(COASSEMBLY("{patient}"),           patient=PATIENT_IDS),
        expand(FILTERED_CONTIGS("{patient}"),     patient=PATIENT_IDS),
        expand(STB_FILE("{patient}"),             patient=PATIENT_IDS),
        expand(BAKTA_TSV("{patient}"),            patient=PATIENT_IDS),
        expand(CHECKM2_OUT("{patient}"),          patient=PATIENT_IDS),
        expand(GTDBTK_OUT("{patient}"),           patient=PATIENT_IDS),
#        expand(DEPTH_FILE("{patient}"),           patient=PATIENT_IDS),
        expand(BIN_TXT("{patient}"),              patient=PATIENT_IDS),
        expand(MUTATION_TYPES("{patient}"),       patient=PATIENT_IDS),
        os.path.join(INSTR_DIRECT, "combined_trend_mapped_type.txt"),
        os.path.join(INSTR_DIRECT, "combined_processed_scaffolds.txt"),
        # per-sample outputs
        lambda wc: all_flagstats(),
        lambda wc: [SNV_FILE(p, s) for p in PATIENT_IDS for s in read_samples(p)],
        lambda wc: [SCF_FILE(p, s) for p in PATIENT_IDS for s in read_samples(p)],
        # merged InStrain tables
        expand(os.path.join(INSTR_DIR("{patient}"), "combined_scaffold_info.tsv"),
               patient=PATIENT_IDS),
        expand(os.path.join(INSTR_DIR("{patient}"), "combined_SNV_info.tsv"),
               patient=PATIENT_IDS)
